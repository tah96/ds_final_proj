[
  {
    "objectID": "EDA.html",
    "href": "EDA.html",
    "title": "Exploratory Analysis",
    "section": "",
    "text": "The purpose of this document is to showcase our exploration of models for classifying people as diabetic based on a variety of factors.Our data comes from Behavioral Risk Factor Surveillance System data set on Kaggle where the organization phoned over 400,000 individuals in 2015 asking about the presence of diabetes and other factors.\nThe purpose of our exploratory data analysis is to explore some possible connections with respect to our response variable (diabetes). We will not review all variables in this document. Those included in our analyses are:\n\nHighBP (High Blood Pressure flag)\nHighChol (High Cholesterol flag)\nAge (Age Brackets)\nGenHlth (General Health Category)\nSex (Female/Male)\nPhysHlth (# days Physical exercise past 30 days)\nMntlHlth\nEducation (Category of greatest education completed)\nIncome (Income bracket)\nSmoker (Smoker flag)\n\nWe aim to correctly classify individuals as diabetic (or not) based on these variables and understand our most important/impactful variables. In real life, this could be used to predict diabetes or those at risk of developing diabetes. We will review a series of models and pick our best one!"
  },
  {
    "objectID": "EDA.html#introduction",
    "href": "EDA.html#introduction",
    "title": "Exploratory Analysis",
    "section": "",
    "text": "The purpose of this document is to showcase our exploration of models for classifying people as diabetic based on a variety of factors.Our data comes from Behavioral Risk Factor Surveillance System data set on Kaggle where the organization phoned over 400,000 individuals in 2015 asking about the presence of diabetes and other factors.\nThe purpose of our exploratory data analysis is to explore some possible connections with respect to our response variable (diabetes). We will not review all variables in this document. Those included in our analyses are:\n\nHighBP (High Blood Pressure flag)\nHighChol (High Cholesterol flag)\nAge (Age Brackets)\nGenHlth (General Health Category)\nSex (Female/Male)\nPhysHlth (# days Physical exercise past 30 days)\nMntlHlth\nEducation (Category of greatest education completed)\nIncome (Income bracket)\nSmoker (Smoker flag)\n\nWe aim to correctly classify individuals as diabetic (or not) based on these variables and understand our most important/impactful variables. In real life, this could be used to predict diabetes or those at risk of developing diabetes. We will review a series of models and pick our best one!"
  },
  {
    "objectID": "EDA.html#libraries-reading-data",
    "href": "EDA.html#libraries-reading-data",
    "title": "Exploratory Analysis",
    "section": "Libraries & Reading Data",
    "text": "Libraries & Reading Data\nBelow we will read in our libraries and our diabetes data set. As we can see in the output, some of our “numeric” variables shouldn’t be numeric as they are categorical variables.\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nraw_data &lt;- read_csv(file='diabetes_data.csv',show_col_types=FALSE)\nhead(raw_data)\n\n# A tibble: 6 × 22\n  Diabetes_binary HighBP HighChol CholCheck   BMI Smoker Stroke\n            &lt;dbl&gt;  &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;\n1               0      1        1         1    40      1      0\n2               0      0        0         0    25      1      0\n3               0      1        1         1    28      0      0\n4               0      1        0         1    27      0      0\n5               0      1        1         1    24      0      0\n6               0      1        1         1    25      1      0\n# ℹ 15 more variables: HeartDiseaseorAttack &lt;dbl&gt;, PhysActivity &lt;dbl&gt;,\n#   Fruits &lt;dbl&gt;, Veggies &lt;dbl&gt;, HvyAlcoholConsump &lt;dbl&gt;, AnyHealthcare &lt;dbl&gt;,\n#   NoDocbcCost &lt;dbl&gt;, GenHlth &lt;dbl&gt;, MentHlth &lt;dbl&gt;, PhysHlth &lt;dbl&gt;,\n#   DiffWalk &lt;dbl&gt;, Sex &lt;dbl&gt;, Age &lt;dbl&gt;, Education &lt;dbl&gt;, Income &lt;dbl&gt;"
  },
  {
    "objectID": "EDA.html#data-cleaning",
    "href": "EDA.html#data-cleaning",
    "title": "Exploratory Analysis",
    "section": "Data Cleaning",
    "text": "Data Cleaning\nAs stated, a lot of our data is labeled as a numeric type but should be categorical. Lets change all those for factors, and for everything that is not binary we will provide labels explicitly.\n\ndata &lt;- raw_data %&gt;%\n  mutate(Diabetes_binary = factor(Diabetes_binary,levels=c(0,1),labels=c('Not Diabetic','Diabetic')),\n         HighBP = factor(HighBP,levels=c(0,1),labels=c('No High BP','High BP')),\n         HighChol = factor(HighChol,levels=c(0,1),labels=c('No High Cholesterol','High Cholesterol')),\n         Smoker = factor(Smoker,levels=c(0,1),labels=c('Not a Smoker','Smoker')),\n         GenHlth = factor(GenHlth,levels=seq(1:5),labels=c('Excellent','Very Good','Good','Fair','Poor')),\n         #DiffWalk = as.factor(DiffWalk),\n         Sex = factor(Sex,levels=c(0,1),labels=c('Female','Male')),\n         Age = factor(Age,levels=seq(1:13),labels=c('18-24','25-29','30-34','35-39','40-44','45-49'\n                                            ,'50-54','55-59','60-64','65-69','70-74','75-79','80+')\n          ),\n         Education = factor(Education,levels=seq(1:6),labels=c('Never attended school or only kindergarten','Grades 1-8','Grades 9-11','Grade 12 or GED','College 1-3 years','College 4+ years')),\n         Income = factor(Income,levels=seq(1:8),labels=c('&lt;$10k','&lt;$15k','&lt;$20k','&lt;$25k','&lt;$35k','&lt;$50k','&lt;$75k','$75k+'))\n    )\n\nhead(data)\n\n# A tibble: 6 × 22\n  Diabetes_binary HighBP     HighChol            CholCheck   BMI Smoker   Stroke\n  &lt;fct&gt;           &lt;fct&gt;      &lt;fct&gt;                   &lt;dbl&gt; &lt;dbl&gt; &lt;fct&gt;     &lt;dbl&gt;\n1 Not Diabetic    High BP    High Cholesterol            1    40 Smoker        0\n2 Not Diabetic    No High BP No High Cholesterol         0    25 Smoker        0\n3 Not Diabetic    High BP    High Cholesterol            1    28 Not a S…      0\n4 Not Diabetic    High BP    No High Cholesterol         1    27 Not a S…      0\n5 Not Diabetic    High BP    High Cholesterol            1    24 Not a S…      0\n6 Not Diabetic    High BP    High Cholesterol            1    25 Smoker        0\n# ℹ 15 more variables: HeartDiseaseorAttack &lt;dbl&gt;, PhysActivity &lt;dbl&gt;,\n#   Fruits &lt;dbl&gt;, Veggies &lt;dbl&gt;, HvyAlcoholConsump &lt;dbl&gt;, AnyHealthcare &lt;dbl&gt;,\n#   NoDocbcCost &lt;dbl&gt;, GenHlth &lt;fct&gt;, MentHlth &lt;dbl&gt;, PhysHlth &lt;dbl&gt;,\n#   DiffWalk &lt;dbl&gt;, Sex &lt;fct&gt;, Age &lt;fct&gt;, Education &lt;fct&gt;, Income &lt;fct&gt;\n\n\nNow that we’ve converted some things to factors, lets check for nulls (NA). There are none! Great news, since we know that not accounting for nulls can hurt our data quality and modeling.\n\ncolSums(is.na(data))\n\n     Diabetes_binary               HighBP             HighChol \n                   0                    0                    0 \n           CholCheck                  BMI               Smoker \n                   0                    0                    0 \n              Stroke HeartDiseaseorAttack         PhysActivity \n                   0                    0                    0 \n              Fruits              Veggies    HvyAlcoholConsump \n                   0                    0                    0 \n       AnyHealthcare          NoDocbcCost              GenHlth \n                   0                    0                    0 \n            MentHlth             PhysHlth             DiffWalk \n                   0                    0                    0 \n                 Sex                  Age            Education \n                   0                    0                    0 \n              Income \n                   0"
  },
  {
    "objectID": "EDA.html#data-summaries",
    "href": "EDA.html#data-summaries",
    "title": "Exploratory Analysis",
    "section": "Data Summaries",
    "text": "Data Summaries\nExploring our data is important. In this section, we’ll take a look at our categorical and numeric variables and trends with respect to our outcome (presence of diabetes). This will include tables and plots.Please note that not all variables are included in our preliminary analyses\n\nCategorical\nStarting with some contingency tables, lets look at two-way contingency tables. Based on what I know, I think that our Age variable (age brackets), HighBP (high blood pressure), and Highhol (high cholesterol) may provide some insights.\nFrom the below outputs we see that\n\nIt is difficult to observe trends in Age bracket and the presence of Diabetes, but we can roughly tell the distribution of our Age brackets. We’ll explore this further in graphs.\nThe majority of those that have diabetes have High Blood Pressure\nThe majority of those that have diabetes have High Cholesterol\nAt surface level, Sex seems to be of equal sample proportions for those with and without diabetes\n\n\ndata %&gt;%\n  group_by(Age,Diabetes_binary) %&gt;%\n  summarize(count = n())\n\n`summarise()` has grouped output by 'Age'. You can override using the `.groups`\nargument.\n\n\n# A tibble: 26 × 3\n# Groups:   Age [13]\n   Age   Diabetes_binary count\n   &lt;fct&gt; &lt;fct&gt;           &lt;int&gt;\n 1 18-24 Not Diabetic     5622\n 2 18-24 Diabetic           78\n 3 25-29 Not Diabetic     7458\n 4 25-29 Diabetic          140\n 5 30-34 Not Diabetic    10809\n 6 30-34 Diabetic          314\n 7 35-39 Not Diabetic    13197\n 8 35-39 Diabetic          626\n 9 40-44 Not Diabetic    15106\n10 40-44 Diabetic         1051\n# ℹ 16 more rows\n\ndata %&gt;%\n  group_by(Diabetes_binary,HighBP) %&gt;%\n  summarize(count = n())\n\n`summarise()` has grouped output by 'Diabetes_binary'. You can override using\nthe `.groups` argument.\n\n\n# A tibble: 4 × 3\n# Groups:   Diabetes_binary [2]\n  Diabetes_binary HighBP      count\n  &lt;fct&gt;           &lt;fct&gt;       &lt;int&gt;\n1 Not Diabetic    No High BP 136109\n2 Not Diabetic    High BP     82225\n3 Diabetic        No High BP   8742\n4 Diabetic        High BP     26604\n\ndata %&gt;%\n  group_by(Diabetes_binary,HighChol) %&gt;%\n  summarize(count = n())\n\n`summarise()` has grouped output by 'Diabetes_binary'. You can override using\nthe `.groups` argument.\n\n\n# A tibble: 4 × 3\n# Groups:   Diabetes_binary [2]\n  Diabetes_binary HighChol             count\n  &lt;fct&gt;           &lt;fct&gt;                &lt;int&gt;\n1 Not Diabetic    No High Cholesterol 134429\n2 Not Diabetic    High Cholesterol     83905\n3 Diabetic        No High Cholesterol  11660\n4 Diabetic        High Cholesterol     23686\n\ndata %&gt;%\n  group_by(Diabetes_binary,Sex) %&gt;%\n  summarize(count = n())\n\n`summarise()` has grouped output by 'Diabetes_binary'. You can override using\nthe `.groups` argument.\n\n\n# A tibble: 4 × 3\n# Groups:   Diabetes_binary [2]\n  Diabetes_binary Sex     count\n  &lt;fct&gt;           &lt;fct&gt;   &lt;int&gt;\n1 Not Diabetic    Female 123563\n2 Not Diabetic    Male    94771\n3 Diabetic        Female  18411\n4 Diabetic        Male    16935\n\n\nNow for some three-way contingency tables. From the above, we saw that people with diabetes exhibited High Blood Pressure OR High Cholesterol. We want to see if that can also be an AND statement. Furthermore, I’m curious on if being a smoker and an assessment of general health has an impact.\nFrom the below outputs we observe…\n\nHigh Cholesterol AND High Blood Pressure is often seen in those with diabetes\nThose without diabetes exhibit Very Good general health (mode) with perhaps a slight penalty for being a smoker.\nThose with diabetes seem to have slightly lower general health of Good with a slight shift in distribution towards Fair if they smoke\n\n\ndata %&gt;%\n  group_by(Diabetes_binary,HighChol,HighBP) %&gt;%\n  summarize(count = n())\n\n`summarise()` has grouped output by 'Diabetes_binary', 'HighChol'. You can\noverride using the `.groups` argument.\n\n\n# A tibble: 8 × 4\n# Groups:   Diabetes_binary, HighChol [4]\n  Diabetes_binary HighChol            HighBP     count\n  &lt;fct&gt;           &lt;fct&gt;               &lt;fct&gt;      &lt;int&gt;\n1 Not Diabetic    No High Cholesterol No High BP 97651\n2 Not Diabetic    No High Cholesterol High BP    36778\n3 Not Diabetic    High Cholesterol    No High BP 38458\n4 Not Diabetic    High Cholesterol    High BP    45447\n5 Diabetic        No High Cholesterol No High BP  4269\n6 Diabetic        No High Cholesterol High BP     7391\n7 Diabetic        High Cholesterol    No High BP  4473\n8 Diabetic        High Cholesterol    High BP    19213\n\ndata %&gt;%\n  group_by(Diabetes_binary,Smoker,GenHlth) %&gt;%\n  summarize(count = n())\n\n`summarise()` has grouped output by 'Diabetes_binary', 'Smoker'. You can\noverride using the `.groups` argument.\n\n\n# A tibble: 20 × 4\n# Groups:   Diabetes_binary, Smoker [4]\n   Diabetes_binary Smoker       GenHlth   count\n   &lt;fct&gt;           &lt;fct&gt;        &lt;fct&gt;     &lt;int&gt;\n 1 Not Diabetic    Not a Smoker Excellent 29755\n 2 Not Diabetic    Not a Smoker Very Good 49460\n 3 Not Diabetic    Not a Smoker Good      32877\n 4 Not Diabetic    Not a Smoker Fair       9627\n 5 Not Diabetic    Not a Smoker Poor       2509\n 6 Not Diabetic    Smoker       Excellent 14404\n 7 Not Diabetic    Smoker       Very Good 33243\n 8 Not Diabetic    Smoker       Good      29312\n 9 Not Diabetic    Smoker       Fair      12153\n10 Not Diabetic    Smoker       Poor       4994\n11 Diabetic        Not a Smoker Excellent   594\n12 Diabetic        Not a Smoker Very Good  3408\n13 Diabetic        Not a Smoker Good       6822\n14 Diabetic        Not a Smoker Fair       4475\n15 Diabetic        Not a Smoker Poor       1730\n16 Diabetic        Smoker       Excellent   546\n17 Diabetic        Smoker       Very Good  2973\n18 Diabetic        Smoker       Good       6635\n19 Diabetic        Smoker       Fair       5315\n20 Diabetic        Smoker       Poor       2848\n\n\nNow for some graphs! To start out, we made a statement earlier that it was difficult to infer anything from our Age and Diabetes contingency table earlier. Lets visualize this graphically. We see in the below that our distribution is left skewed for our respondents and it seems that there is little to know impact of age on the precense of diabetes.\n\nggplot(data,\n       aes(x=Age,fill=Diabetes_binary)\n       ) +\n  geom_bar() +\n  scale_fill_discrete(\"Diabetes\") +\n  labs(y = 'Frequency',title='Age Distribution colored by Diabetes')\n\n\n\n\nSimilar to Age, lets create similar bar graphs for levels of income and education. It is our hypothesis that there is no connection or obvious patterns here. Lets see what it looks like at first glance\nOur hypothesis appears correct.\n\nggplot(data,\n       aes(x=Income,fill=Diabetes_binary)\n       ) +\n  geom_bar() +\n  scale_fill_discrete(\"Diabetes\") +\n  labs(y = 'Frequency',title='Income Distribution colored by Diabetes')\n\n\n\nggplot(data,\n       aes(x=Education,fill=Diabetes_binary)\n       ) +\n  geom_bar() +\n  scale_fill_discrete(\"Diabetes\") +\n  labs(y='Frequency',title='Income Distribution colored by Diabetes')\n\n\n\n\nMaybe we can find some patterns using faceting. Lets build a series of bar graphs with GenHealth on our X-Axis colored by the presence of diabetes. Lets facet this by our age brackets to see if there’s a pattern.\nThe output isn’t super friendly, but because we know our General Health is a factor type the axis goes from left-to-right in the following order: Excellent,Very Good,Good,Fair,Poor\nLooking at General Health alone, it seems there’s a slight right skew that becomes more normal with age. Its difficult to tell, but there may be some suggestion that your general health leads to an increased risk of diabetes due to the eye-level proportions.\n\nggplot(data,\n       aes(x=GenHlth,fill=Diabetes_binary)\n       ) +\n  geom_bar() +\n  scale_fill_discrete(\"Diabetes\") +\n  labs(x='General Health',y = 'Frequency',title='General Health by Age Bracket') +\n  facet_wrap(~Age)\n\n\n\n\n\n\nNumeric\nWe have a lot less numeric variables here to analyze, but there still may be some important details in our summaries. Lets look at our distributions and measures of spread and central tendency.\nFor starters, lets look at standalone variables. Lets look at some stats for BMI,MentHlth,PhysHlth.\nBelow are some observations:\n\nOur mean and median BMI values are between 27-29 with a standard deviation of 6. With a slight left skew, both values fall into the “Overweight” category.\nThe median number of days (over past 30 days) our participants expressed having some mental health problems is 0. This is a bit odd and may showcase some bias in how our participants evaluate themselves.\nYikes! The median number of days (over past 30 days) our participants expressed participating in physical activities is also 0! Not a very active bunch!\n\n\ndata %&gt;%\n  summarize(\"mean\" = mean(BMI),\n            \"median\" = median(BMI),\n            \"var\" = var(BMI),\n            \"sd\" = sd(BMI),\n            \"IQR\" = IQR(BMI)\n            )\n\n# A tibble: 1 × 5\n   mean median   var    sd   IQR\n  &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1  28.4     27  43.7  6.61     7\n\ndata %&gt;%\n  summarize(\"mean\" = mean(MentHlth),\n            \"median\" = median(MentHlth),\n            \"var\" = var(MentHlth),\n            \"sd\" = sd(MentHlth),\n            \"IQR\" = IQR(MentHlth)\n            )\n\n# A tibble: 1 × 5\n   mean median   var    sd   IQR\n  &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1  3.18      0  55.0  7.41     2\n\ndata %&gt;%\n  summarize(\"mean\" = mean(PhysHlth),\n            \"median\" = median(PhysHlth),\n            \"var\" = var(PhysHlth),\n            \"sd\" = sd(PhysHlth),\n            \"IQR\" = IQR(PhysHlth)\n            )\n\n# A tibble: 1 × 5\n   mean median   var    sd   IQR\n  &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1  4.24      0  76.0  8.72     3\n\n\nLets go a step beyond one-variable summaries here. Mental Health and Physical Health seem that they may have their own problems. Fortunately for us, we have a BMI number. We would expect our measures of central tendency to be higher, on average, for those with diabetes.\nOur hypothesis seems correct at first glance with our median BMI being 27 (Overweight) for those without diabetes and 31 (Obese) for those with diabetes. We’ll explore this more in our graphs.\n\ndata %&gt;%\n  select(Diabetes_binary,BMI) %&gt;%\n  group_by(Diabetes_binary) %&gt;%\n  summarize(across(everything(), .fns = list(\"mean\" = mean,\n                                       \"median\" = median,\n                                       \"var\" = var,\n                                       \"sd\" = sd,\n                                       \"IQR\" = IQR), .names = \"{.fn}_{.col}\"))\n\n# A tibble: 2 × 6\n  Diabetes_binary mean_BMI median_BMI var_BMI sd_BMI IQR_BMI\n  &lt;fct&gt;              &lt;dbl&gt;      &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt;\n1 Not Diabetic        27.8         27    39.6   6.29       7\n2 Diabetic            31.9         31    54.2   7.36       8\n\n\nLets graph out the above relationship on a density chart. We can certainly see a shift to the right (higher BMI) for those with diabetes.\n\nbmi_plot &lt;- ggplot(data,\n       aes(x=BMI,fill=Diabetes_binary)\n       ) +\n  geom_density(alpha=0.5,kernel=\"gaussian\") +\n  scale_fill_discrete(\"Diabetes\") +\n  labs(x='BMI',y='Frequency')\n\nbmi_plot +\n  labs(title='BMI')\n\n\n\n\nIt would be interesting to see this faceted across our categorical HighChol and HighBp measures. We would expect to see some patterns.\nIt is difficult to tell in the graphs, but the graphs look similar across low and high levels of each categorical. There may be a slight shift to the right (BMI) in measures of central tendency across high levels of each categorical. We’ll uncover more on the impact of this in our modeling!\n\nbmi_plot +\n  labs(title = 'BMI Distribution Across Generic Cholesterol Levels') +\n  facet_wrap(~HighChol)\n\n\n\nbmi_plot +\n  labs(title = 'BMI Distribution Across Generic Blood Pressure Levels') +\n  facet_wrap(~HighBP)\n\n\n\n\nFrom our previous analyses, I hypothesize that mental health and physical health have little to no influence, and its hard to think a box plot or violin plot will provide much information.\nWhat about our correlation between our numeric variables.? It seems BMI and our numeric health metrics have little correlation, but we see low-to-moderate positive correlation between MentHlth and PhysHlth.\n\ncorrelations &lt;- summary(correlation::correlation(data %&gt;% select(BMI, MentHlth, PhysHlth)))\n\nplot(correlations, show_data = \"points\")"
  },
  {
    "objectID": "EDA.html#summary",
    "href": "EDA.html#summary",
    "title": "Exploratory Analysis",
    "section": "Summary",
    "text": "Summary\nThat concludes our exploratory data analysis. We’ve seen a lot of interesting trends or absence of trends. Generally, here are some highlights:\n\nBMI seems to be higher for those with diabetes\nMental Health and Physical Health measures seem to have some correlation, but none that seem to impact BMI or classification of someone having diabetes.\nGeneral Health may have some impact on developing diabetes.\nHigh Blood Pressure and/or High Cholesterol are relatively more common in those with diabetes than those without.\n\nTo explore this further with modeling please see Click here for the Modeling Page"
  },
  {
    "objectID": "Modeling.html",
    "href": "Modeling.html",
    "title": "Modeling",
    "section": "",
    "text": "The purpose of this document is to showcase our exploration of models for classifying people as diabetic based on a variety of factors.Our data comes from Behavioral Risk Factor Surveillance System data set on Kaggle where the organization phoned over 400,000 individuals in 2015 asking about the presence of diabetes and other factors.\nWe aim to correctly classify individuals as diabetic (or not) based on these variables and understand our most important/impactful variables. In real life, this could be used to predict diabetes or those at risk of developing diabetes. We will review a series of models and pick our best one!"
  },
  {
    "objectID": "Modeling.html#introduction",
    "href": "Modeling.html#introduction",
    "title": "Modeling",
    "section": "",
    "text": "The purpose of this document is to showcase our exploration of models for classifying people as diabetic based on a variety of factors.Our data comes from Behavioral Risk Factor Surveillance System data set on Kaggle where the organization phoned over 400,000 individuals in 2015 asking about the presence of diabetes and other factors.\nWe aim to correctly classify individuals as diabetic (or not) based on these variables and understand our most important/impactful variables. In real life, this could be used to predict diabetes or those at risk of developing diabetes. We will review a series of models and pick our best one!"
  },
  {
    "objectID": "Modeling.html#libraries-reading-data",
    "href": "Modeling.html#libraries-reading-data",
    "title": "Modeling",
    "section": "Libraries & Reading Data",
    "text": "Libraries & Reading Data\nBelow we will read in our libraries and our diabetes data set. As we can see in the output, some of our “numeric” variables shouldn’t be numeric as they are categorical variables. If our categorical variables are binary, we will leave as is for easier modeling.\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(tidymodels)\n\n── Attaching packages ────────────────────────────────────── tidymodels 1.2.0 ──\n✔ broom        1.0.7     ✔ rsample      1.2.1\n✔ dials        1.3.0     ✔ tune         1.2.1\n✔ infer        1.0.7     ✔ workflows    1.1.4\n✔ modeldata    1.4.0     ✔ workflowsets 1.1.0\n✔ parsnip      1.2.1     ✔ yardstick    1.3.1\n✔ recipes      1.1.0     \n── Conflicts ───────────────────────────────────────── tidymodels_conflicts() ──\n✖ scales::discard() masks purrr::discard()\n✖ dplyr::filter()   masks stats::filter()\n✖ recipes::fixed()  masks stringr::fixed()\n✖ dplyr::lag()      masks stats::lag()\n✖ yardstick::spec() masks readr::spec()\n✖ recipes::step()   masks stats::step()\n• Search for functions across packages at https://www.tidymodels.org/find/\n\nlibrary(rpart)\n\n\nAttaching package: 'rpart'\n\nThe following object is masked from 'package:dials':\n\n    prune\n\nlibrary(ranger)\nlibrary(vip)\n\n\nAttaching package: 'vip'\n\nThe following object is masked from 'package:utils':\n\n    vi\n\nraw_data &lt;- read_csv(file='diabetes_data.csv',show_col_types=FALSE)\nhead(raw_data)\n\n# A tibble: 6 × 22\n  Diabetes_binary HighBP HighChol CholCheck   BMI Smoker Stroke\n            &lt;dbl&gt;  &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;\n1               0      1        1         1    40      1      0\n2               0      0        0         0    25      1      0\n3               0      1        1         1    28      0      0\n4               0      1        0         1    27      0      0\n5               0      1        1         1    24      0      0\n6               0      1        1         1    25      1      0\n# ℹ 15 more variables: HeartDiseaseorAttack &lt;dbl&gt;, PhysActivity &lt;dbl&gt;,\n#   Fruits &lt;dbl&gt;, Veggies &lt;dbl&gt;, HvyAlcoholConsump &lt;dbl&gt;, AnyHealthcare &lt;dbl&gt;,\n#   NoDocbcCost &lt;dbl&gt;, GenHlth &lt;dbl&gt;, MentHlth &lt;dbl&gt;, PhysHlth &lt;dbl&gt;,\n#   DiffWalk &lt;dbl&gt;, Sex &lt;dbl&gt;, Age &lt;dbl&gt;, Education &lt;dbl&gt;, Income &lt;dbl&gt;"
  },
  {
    "objectID": "Modeling.html#data-cleaning",
    "href": "Modeling.html#data-cleaning",
    "title": "Modeling",
    "section": "Data Cleaning",
    "text": "Data Cleaning\nAs stated, a lot of our data is labeled as a numeric type but should be categorical. Lets change all those for factors, and for everything that is not binary we will provide labels explicitly.\n\ndata &lt;- raw_data %&gt;%\n  mutate(Diabetes_binary = factor(Diabetes_binary,levels=c(0,1),labels=c('Not Diabetic','Diabetic')),\n         GenHlth = factor(GenHlth,levels=seq(1:5),labels=c('Excellent','Very Good','Good','Fair','Poor')),\n         Age = factor(Age,levels=seq(1:13),labels=c('18-24','25-29','30-34','35-39','40-44','45-49'\n                                            ,'50-54','55-59','60-64','65-69','70-74','75-79','80+')\n          ),\n         Education = factor(Education,levels=seq(1:6),labels=c('Never attended school or only kindergarten','Grades 1-8','Grades 9-11','Grade 12 or GED','College 1-3 years','College 4+ years')),\n         Income = factor(Income,levels=seq(1:8),labels=c('&lt;$10k','&lt;$15k','&lt;$20k','&lt;$25k','&lt;$35k','&lt;$50k','&lt;$75k','$75k+'))\n    )\n\nhead(data)\n\n# A tibble: 6 × 22\n  Diabetes_binary HighBP HighChol CholCheck   BMI Smoker Stroke\n  &lt;fct&gt;            &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;\n1 Not Diabetic         1        1         1    40      1      0\n2 Not Diabetic         0        0         0    25      1      0\n3 Not Diabetic         1        1         1    28      0      0\n4 Not Diabetic         1        0         1    27      0      0\n5 Not Diabetic         1        1         1    24      0      0\n6 Not Diabetic         1        1         1    25      1      0\n# ℹ 15 more variables: HeartDiseaseorAttack &lt;dbl&gt;, PhysActivity &lt;dbl&gt;,\n#   Fruits &lt;dbl&gt;, Veggies &lt;dbl&gt;, HvyAlcoholConsump &lt;dbl&gt;, AnyHealthcare &lt;dbl&gt;,\n#   NoDocbcCost &lt;dbl&gt;, GenHlth &lt;fct&gt;, MentHlth &lt;dbl&gt;, PhysHlth &lt;dbl&gt;,\n#   DiffWalk &lt;dbl&gt;, Sex &lt;dbl&gt;, Age &lt;fct&gt;, Education &lt;fct&gt;, Income &lt;fct&gt;"
  },
  {
    "objectID": "Modeling.html#training-test-and-folds",
    "href": "Modeling.html#training-test-and-folds",
    "title": "Modeling",
    "section": "Training, Test and Folds",
    "text": "Training, Test and Folds\nWe want to split our data into a 70% training and 30% test set. For fitting our models, we also want 5 folds from our training set for cross-validation.\nWe’ll set a seed as well to make sure our results, and comments about those results, are reproducible.\nThe below output is of our split showing ~178k records into our training set and ~76k records in our test set.\n\nset.seed(123)\n\n#data &lt;- data[1:10000,]\ndata_split &lt;- initial_split(data,prop=0.70)\ndata_train &lt;- training(data_split)\ndata_test &lt;- testing(data_split)\ndata_folds &lt;- vfold_cv(data_train,5)\n\ndata_split\n\n&lt;Training/Testing/Total&gt;\n&lt;177576/76104/253680&gt;"
  },
  {
    "objectID": "Modeling.html#models",
    "href": "Modeling.html#models",
    "title": "Modeling",
    "section": "Models",
    "text": "Models\nThis section will hold both models considered: A Classification Tree and a Random Forest. We will train and evaluate these models and declare our winner that we’ll use in later parts of this project\n\nClassification Tree\nA classification tree is a tree-based model aimed at placing subjects (classifying) into a single category (outcome) based on what we know about them (inputs).\nThe tree starts with a node and uses log-loss to determine a split (branches). After that split it goes to each child leaf node and repeats the process as many times as necessary or as many times as declared by the modeler.\nRecipe\nFirst we’ll construct our recipe. For this we’ll concentrate on the variables BMI,HighBP,HighChol,GenHlth,Sex,Smoker,Stroke,HvyAlcoholConsump,DiffWalk,HeartDiseaseorAttack.\nWe’ll normalize our one numeric variable BMI. We’ll also add dummy variables for our one factor variable with multiple levels GenHlth. There is no need to update anything to an ID type because that does not exist in the data set.\n\nclass_recipe &lt;- recipe(Diabetes_binary ~ BMI + HighBP + HighChol + GenHlth + Sex + Smoker + Stroke + HvyAlcoholConsump + DiffWalk + HeartDiseaseorAttack\n                         , data = data_train) |&gt;\n  step_normalize(BMI) |&gt;\n  step_dummy(GenHlth)\n\nclass_recipe\n\n\n\n\n── Recipe ──────────────────────────────────────────────────────────────────────\n\n\n\n\n\n── Inputs \n\n\nNumber of variables by role\n\n\noutcome:    1\npredictor: 10\n\n\n\n\n\n── Operations \n\n\n• Centering and scaling for: BMI\n\n\n• Dummy variables from: GenHlth\n\n\nModel & Engine\nNow to set up our model and engine. We know that this is a tree model and we want to tune on our cost_complexity parameter. Its important we set our mode to classification here, since we are predicting a classification!\n\nclass_mod &lt;- decision_tree(tree_depth = 5,\n                          min_n = 5,\n                          cost_complexity = tune()) |&gt;\n  set_engine(\"rpart\") |&gt;\n  set_mode(\"classification\")\n\nclass_mod\n\nDecision Tree Model Specification (classification)\n\nMain Arguments:\n  cost_complexity = tune()\n  tree_depth = 5\n  min_n = 5\n\nComputational engine: rpart \n\n\nWorkflow & First-Fit\nLets put it all into a workflow and collect our metrics (log-loss). We’ll tune our grid to our resamples on our cross-validation folds and limit the tuning of our cost complexity hyperparameter to 15 levels.\n\nclass_wfl &lt;- workflow() |&gt;\n  add_recipe(class_recipe) |&gt;\n  add_model(class_mod)\n\nclass_fits &lt;- class_wfl |&gt; \n  tune_grid(resamples = data_folds,\n            grid = grid_regular(cost_complexity(),\n                                levels = 15),\n            metrics = metric_set(mn_log_loss)\n            )\n\nclass_fits |&gt;\n  collect_metrics()\n\n# A tibble: 15 × 7\n   cost_complexity .metric     .estimator  mean     n  std_err .config          \n             &lt;dbl&gt; &lt;chr&gt;       &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;    &lt;dbl&gt; &lt;chr&gt;            \n 1        1   e-10 mn_log_loss binary     0.357     5 0.000816 Preprocessor1_Mo…\n 2        4.39e-10 mn_log_loss binary     0.357     5 0.000816 Preprocessor1_Mo…\n 3        1.93e- 9 mn_log_loss binary     0.357     5 0.000816 Preprocessor1_Mo…\n 4        8.48e- 9 mn_log_loss binary     0.357     5 0.000816 Preprocessor1_Mo…\n 5        3.73e- 8 mn_log_loss binary     0.357     5 0.000816 Preprocessor1_Mo…\n 6        1.64e- 7 mn_log_loss binary     0.357     5 0.000816 Preprocessor1_Mo…\n 7        7.20e- 7 mn_log_loss binary     0.357     5 0.000816 Preprocessor1_Mo…\n 8        3.16e- 6 mn_log_loss binary     0.357     5 0.000816 Preprocessor1_Mo…\n 9        1.39e- 5 mn_log_loss binary     0.357     5 0.000816 Preprocessor1_Mo…\n10        6.11e- 5 mn_log_loss binary     0.357     5 0.000816 Preprocessor1_Mo…\n11        2.68e- 4 mn_log_loss binary     0.357     5 0.000816 Preprocessor1_Mo…\n12        1.18e- 3 mn_log_loss binary     0.357     5 0.000842 Preprocessor1_Mo…\n13        5.18e- 3 mn_log_loss binary     0.403     5 0.000693 Preprocessor1_Mo…\n14        2.28e- 2 mn_log_loss binary     0.403     5 0.000693 Preprocessor1_Mo…\n15        1   e- 1 mn_log_loss binary     0.403     5 0.000693 Preprocessor1_Mo…\n\n\nTest Measure\nWe’ll use our the value for cost_complexity that led to the lowest mn_log_loss value. We’ll use this to finalize our workflow.\nWith this finalized workflow, we’ll fit the data on our entire training set and evaluate against the test set keeping log-loss our our metric.\nBelow you can see the metrics of our model against the test set. This will come in handy later when selecting our model. We see here our log-loss is estimated at 0.356.\n\nclass_best_params &lt;- select_best(class_fits, metric = \"mn_log_loss\")\n\nclass_final_wfl &lt;- class_wfl |&gt;\n  finalize_workflow(class_best_params)\n\nclass_final_fit &lt;- class_final_wfl |&gt;\n  last_fit(data_split,metrics=metric_set(mn_log_loss))\n\nclass_test_metrics &lt;- class_final_fit |&gt;\n  collect_metrics()\n\nclass_test_metrics\n\n# A tibble: 1 × 4\n  .metric     .estimator .estimate .config             \n  &lt;chr&gt;       &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt;               \n1 mn_log_loss binary         0.359 Preprocessor1_Model1\n\n\n\n\nRandom Forest\nThe second model we’ll look at is a random forest model. A random forest model is also a tree-fit model. A random forest model creates a series of tree’s using a random set of predictors on each fold in our cross validation set.\nLets say one on our our folds our random forest produces 200 tree models. Each may have different predictors at each split of the tree. The average CV error will be taken.\nCompared to a basic classification tree, not all predictors are considered and a random forest is an ensemble method (many trees). As a result of not using all predictors, we are careful to reduce risk of over fitting to our training set.\nRecipe & Model\nWe’ll use the same recipe and set of predictors per our classification model. Our model will again be set as a classification model. Unlike our basic classification model, we will tune on our mtry hyperparameter.\n\nrf_model &lt;- rand_forest(mtry = tune()) |&gt;\n  set_engine(\"ranger\", importance = \"impurity\") |&gt;\n  set_mode(\"classification\")\n\nWorkflow & First-Fit\nLets put it all into a workflow and collect our metrics (log-loss). We’ll tune our grid to our resamples on our cross-validation folds and fit a model. Since we’re using the same predictors and outcome, we’ll reuse or recipe from prior steps.\nIt looks like the best value for our mtry hyperparameter is 4.\n\nrf_wfl &lt;- workflow() |&gt;\n  add_recipe(class_recipe) |&gt;\n  add_model(rf_model)\n\nrf_fit &lt;- rf_wfl |&gt;\n  tune_grid(resamples = data_folds,\n            grid = 5,\n            metrics = metric_set(mn_log_loss))\n\ni Creating pre-processing data to finalize unknown parameter: mtry\n\nrf_fit |&gt;\n  collect_metrics()\n\n# A tibble: 5 × 7\n   mtry .metric     .estimator  mean     n  std_err .config             \n  &lt;int&gt; &lt;chr&gt;       &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;    &lt;dbl&gt; &lt;chr&gt;               \n1     5 mn_log_loss binary     0.324     5 0.00103  Preprocessor1_Model1\n2     9 mn_log_loss binary     0.337     5 0.000943 Preprocessor1_Model2\n3     6 mn_log_loss binary     0.326     5 0.00103  Preprocessor1_Model3\n4     3 mn_log_loss binary     0.323     5 0.000947 Preprocessor1_Model4\n5    12 mn_log_loss binary     0.354     5 0.000728 Preprocessor1_Model5\n\n\nTest Measure\nWe’ll use our value for mtry that led to the lowest mn_log_loss value. We’ll use this to finalize our workflow.\nWith this finalized workflow, we’ll fit the data on our entire training set and evaluate against the test set keeping log-loss our our metric.\nBelow you can see the metrics of our model against the test set. This will come in handy later when selecting our model. We see here our log-loss is estimated at 0.324.\n\nrf_best_params &lt;- select_best(rf_fit, metric = \"mn_log_loss\")\n\nrf_final_wfl &lt;- rf_wfl |&gt;\n  finalize_workflow(rf_best_params)\n\nrf_final_fit &lt;- rf_final_wfl |&gt;\n  last_fit(data_split,metrics=metric_set(mn_log_loss))\n\nrf_test_metrics &lt;- rf_final_fit |&gt;\n  collect_metrics()\n\nrf_test_metrics\n\n# A tibble: 1 × 4\n  .metric     .estimator .estimate .config             \n  &lt;chr&gt;       &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt;               \n1 mn_log_loss binary         0.325 Preprocessor1_Model1"
  },
  {
    "objectID": "Modeling.html#selecting-our-model",
    "href": "Modeling.html#selecting-our-model",
    "title": "Modeling",
    "section": "Selecting our model",
    "text": "Selecting our model\nWhich of our best models of each type performed the best? From the below, we can declare the random forest model as our winner!\n\nrbind(class_test_metrics,rf_test_metrics)\n\n# A tibble: 2 × 4\n  .metric     .estimator .estimate .config             \n  &lt;chr&gt;       &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt;               \n1 mn_log_loss binary         0.359 Preprocessor1_Model1\n2 mn_log_loss binary         0.325 Preprocessor1_Model1"
  },
  {
    "objectID": "Modeling.html#final-fit-parameter-estimates",
    "href": "Modeling.html#final-fit-parameter-estimates",
    "title": "Modeling",
    "section": "Final Fit & Parameter estimates",
    "text": "Final Fit & Parameter estimates\nFinally, we’ll fit our model to the whole dataset. For fun, we’ll plot our variable importances.\nWe see High Blood Pressure and High Cholesterol have great importance! General Health categories also have some notable importance.\n\nrf_extract_fit &lt;- rf_final_wfl |&gt;\n  fit(data) |&gt;\n  extract_fit_engine()\n\nrf_extract_fit |&gt;\n  vip::vi() |&gt;\n  arrange(Importance) |&gt;\n  ggplot(aes(x = Variable, y = Importance)) +\n  geom_bar(stat =\"identity\") +\n  coord_flip()"
  }
]